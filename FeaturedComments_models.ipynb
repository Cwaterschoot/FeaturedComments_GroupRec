{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e362900d-20a8-41bb-bb7d-0f6015d2a387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re\n",
    "import random\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sentence_transformers import models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, BertTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import pickle\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13f613f-4b14-4a0e-b7c0-20b8781d116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "#### DATA #######\n",
    "#################\n",
    "\n",
    "\n",
    "df = pd.read_csv('full_2020_bow.csv') ## Full data available on request\n",
    "\n",
    "\n",
    "# Chronological sorting of articles\n",
    "df['date'] = pd.to_datetime(df['article_time'])\n",
    "df = df.sort_values(by='date')\n",
    "\n",
    "import random\n",
    "random.seed(1234)\n",
    "article_ids = np.array(df.article_id)\n",
    "articles = np.unique(article_ids)\n",
    "\n",
    "# Splitting at the halfway point\n",
    "set1 = articles[:1476]\n",
    "set2 = articles[1477:]\n",
    "\n",
    "\n",
    "\n",
    "# Getting row indices for each set of articles in original data\n",
    "index1 = []\n",
    "index2 = []\n",
    "\n",
    "for row in df.itertuples():\n",
    "    if row.article_id in set1:\n",
    "        index1.append(row.Index)\n",
    "    else:\n",
    "        index2.append(row.Index)\n",
    "        \n",
    "df1 = df.loc[index1,:] #### DATASET 1: for train, val, test\n",
    "df2 = df.loc[index2,:] #### DATASET 2: for unseen eval articles\n",
    "\n",
    "## SPLIT DF1 IN TRAIN, TEST, VAL\n",
    "np.random.seed(1)\n",
    "df1 = df1.replace({np.nan:0})\n",
    "\n",
    "train, validate, test = np.split(df1.sample(frac=1), [int(.8*len(df1)), int(.9*len(df1))])\n",
    "\n",
    "print('train:' , collections.Counter(train['featured']))\n",
    "print('validate:' , collections.Counter(validate['featured']))\n",
    "print('test:' , collections.Counter(test['featured']))\n",
    "\n",
    "###\n",
    "train2 = train[train.label==0].sample(97660, random_state=1)  # DEFINE SPLIT:paper 97660, 95-5 split as calculated on validation data\n",
    "train_f2 = train[train.label==1].sample(4883, random_state=1)  # DEFINE SPLIT\n",
    "\n",
    "#train = pd.concat([train2, train_f2]) ## NEW TRAINING DATA\n",
    "train2 = pd.concat([train2, train_f2]) ## NEW TRAINING DATA\n",
    "train_labels = train2.featured\n",
    "\n",
    "### processing for textual models\n",
    "\n",
    "train_texts = list(train2.content)\n",
    "test_texts = list(test.content)\n",
    "val_texts = list(val.content)\n",
    "\n",
    "test_labels = list(test.featured)\n",
    "val_labels= list(validate.featured)\n",
    "test_labels_temp = list(test.featured)\n",
    "val_labels_temp = list(val.featured)\n",
    "\n",
    "train_labels_num = []\n",
    "for i in range(len(train_labels)):\n",
    "    if train_labels[i]==True:\n",
    "        train_labels_num.append(1)\n",
    "    else:\n",
    "        train_labels_num.append(0)\n",
    "\n",
    "\n",
    "val_labels_num = []\n",
    "for i in range(len(val_labels_temp)):\n",
    "    if val_labels_temp[i]==True:\n",
    "        val_labels_num.append(1)\n",
    "    else:\n",
    "        val_labels_num.append(0)\n",
    "\n",
    "\n",
    "test_labels_num = []\n",
    "for i in range(len(test_labels_temp)):\n",
    "    if test_labels_temp[i]==True:\n",
    "        test_labels_num.append(1)\n",
    "    else:\n",
    "        test_labels_num.append(0)\n",
    "\n",
    "\n",
    "y_train = np.array(train_labels_num)\n",
    "y_test = np.array(test_labels_num)\n",
    "y_val = np.array(val_labels_num)\n",
    "\n",
    "\n",
    "labels_df2 = np.array(labels_df2_conv)\n",
    "\n",
    "\n",
    "\n",
    "max_words = 10000  # Maximum number of words in the vocabulary\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "\n",
    "all_texts = list(df.content)\n",
    "tokenizer.fit_on_texts(all_texts)\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(train_texts)\n",
    "sequences_test = tokenizer.texts_to_sequences(test_texts)\n",
    "sequences_val = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_sequence_length = 100  \n",
    "padded_sequences_train = pad_sequences(sequences_train, maxlen=max_sequence_length)\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen=max_sequence_length)\n",
    "padded_sequences_val = pad_sequences(sequences_val, maxlen=max_sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2462b746-65ad-47c8-a22d-d614b2c50d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "################## RF #######################\n",
    "#############################################\n",
    "\n",
    "## Correct_cols is used to make sure the set of variables is consistent between training and ranking\n",
    "correct_cols = [\n",
    " 'reply_count',\n",
    " 'respect_count',\n",
    " 'delta_minutes',\n",
    " 'featured_posts_user',\n",
    " 'rejected_count_user',\n",
    " 'total_respect_user',\n",
    " 'total_posts_user',\n",
    " 'total_received_responses_user',\n",
    " 'ratio_rejected',\n",
    " 'ratio_featured',\n",
    " 'ratio_reply',\n",
    " 'ratio_respect',\n",
    " 'wordcount',\n",
    " 'intro_dist',\n",
    " 'centr_dist',\n",
    " 'avg_wordcount_user'\n",
    "]\n",
    "\n",
    "train_rf = train2[train2.columns.intersection(correct_cols)]\n",
    "validate_rf = validate[validate.columns.intersection(correct_cols)]\n",
    "test_rf = test[test.columns.intersection(correct_cols)]\n",
    "\n",
    "\n",
    "# Defining grid search to be used in all RFs\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "criterion = ['gini', 'entropy']\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap, 'criterion': criterion}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 50, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "rf_random = rf_random.fit(train_rf, train_labels)\n",
    "rf1 = rf_random.best_estimator_\n",
    "\n",
    "\n",
    "###### VALIDATION SET, ONLY USED WITH RF #############################\n",
    "\n",
    "\n",
    "pred = rf1.predict(validate_rf)\n",
    "print('f1 on validation set (RF):',f1_score(validate_labels, list(pred), pos_label=True))\n",
    "print('prec on validation set (RF):',precision_score(validate_labels, list(pred), pos_label=True))\n",
    "print('rec on validation set (RF):',recall_score(validate_labels, list(pred), pos_label=True))\n",
    "######################################################################\n",
    "\n",
    "# Evaluation on Test set\n",
    "pred = rf1.predict(test_rf)\n",
    "print('f1 on test set (RF):',f1_score(test_labels, list(pred), pos_label=True))\n",
    "print('prec on test set (RF):',precision_score(test_labels, list(pred), pos_label=True))\n",
    "print('rec on test set (RF):',recall_score(test_labels, list(pred), pos_label=True))\n",
    "\n",
    "# Save RF\n",
    "with open('rf', 'wb') as a:\n",
    "    pickle.dump(rf1, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b0bc6f-ae5c-4528-9f7b-8e5b41023f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correct cols for BoW\n",
    "# Just to make sure feature set matches during ranking\n",
    "\n",
    "correct_cols = [\n",
    " 'reply_count',\n",
    " 'respect_count',\n",
    " 'featured',\n",
    " 'delta_minutes',\n",
    " 'featured_posts_user',\n",
    " 'rejected_count_user',\n",
    " 'total_respect_user',\n",
    " 'total_posts_user',\n",
    " 'total_received_responses_user',\n",
    " 'ratio_rejected',\n",
    " 'ratio_featured',\n",
    " 'ratio_reply',\n",
    " 'ratio_respect',\n",
    " 'wordcount',\n",
    " 'intro_dist',\n",
    " 'centr_dist',\n",
    " 'avg_wordcount_user',\n",
    "    '000',\n",
    " '10',\n",
    " '100',\n",
    " '20',\n",
    " 'aan het',\n",
    " 'aan te',\n",
    " 'aantal',\n",
    " 'achter',\n",
    " 'acties',\n",
    " 'af',\n",
    " 'afgelopen',\n",
    " 'afstand',\n",
    " 'al die',\n",
    " 'alleen maar',\n",
    " 'allemaal',\n",
    " 'alles',\n",
    " 'als de',\n",
    " 'als een',\n",
    " 'als er',\n",
    " 'als het',\n",
    " 'als ik',\n",
    " 'als je',\n",
    " 'als we',\n",
    " 'als ze',\n",
    " 'altijd',\n",
    " 'ander',\n",
    " 'anderen',\n",
    " 'anders',\n",
    " 'artikel',\n",
    " 'auto',\n",
    " 'bedrijf',\n",
    " 'bedrijven',\n",
    " 'beetje',\n",
    " 'beleid',\n",
    " 'bent',\n",
    " 'besmet',\n",
    " 'besmettingen',\n",
    " 'best',\n",
    " 'beste',\n",
    " 'betalen',\n",
    " 'beter',\n",
    " 'betreft',\n",
    " 'bevolking',\n",
    " 'bezig',\n",
    " 'bij de',\n",
    " 'bijna',\n",
    " 'bijvoorbeeld',\n",
    " 'binnen',\n",
    " 'blijft',\n",
    " 'blijkbaar',\n",
    " 'blijven',\n",
    " 'boer',\n",
    " 'br br',\n",
    " 'br de',\n",
    " 'br div',\n",
    " 'br div div',\n",
    " 'buiten',\n",
    " 'burgers',\n",
    " 'china',\n",
    " 'cijfers',\n",
    " 'corona',\n",
    " 'covid',\n",
    " 'daarnaast',\n",
    " 'daarom',\n",
    " 'dag',\n",
    " 'dagen',\n",
    " 'dan de',\n",
    " 'dan is',\n",
    " 'dan ook',\n",
    " 'dat dit',\n",
    " 'dat een',\n",
    " 'dat er',\n",
    " 'dat hij',\n",
    " 'dat ik',\n",
    " 'dat je',\n",
    " 'dat niet',\n",
    " 'dat we',\n",
    " 'dat ze',\n",
    " 'de maatregelen',\n",
    " 'de meeste',\n",
    " 'de mensen',\n",
    " 'de natuur',\n",
    " 'de overheid',\n",
    " 'de politie',\n",
    " 'de regels',\n",
    " 'de regering',\n",
    " 'de rest',\n",
    " 'de wereld',\n",
    " 'de zorg',\n",
    " 'deel',\n",
    " 'denk',\n",
    " 'denk dat',\n",
    " 'denken',\n",
    " 'denkt',\n",
    " 'dicht',\n",
    " 'die de',\n",
    " 'die het',\n",
    " 'dingen',\n",
    " 'dit is',\n",
    " 'dit soort',\n",
    " 'div br',\n",
    " 'div br div',\n",
    " 'div div',\n",
    " 'div div br',\n",
    " 'doden',\n",
    " 'doe',\n",
    " 'doet',\n",
    " 'door de',\n",
    " 'druk',\n",
    " 'duidelijk',\n",
    " 'duitsland',\n",
    " 'echter',\n",
    " 'economie',\n",
    " 'een beetje',\n",
    " 'een paar',\n",
    " 'eerder',\n",
    " 'eerst',\n",
    " 'eerste',\n",
    " 'eigen',\n",
    " 'eigenlijk',\n",
    " 'elkaar',\n",
    " 'elke',\n",
    " 'en als',\n",
    " 'en dan',\n",
    " 'en dat',\n",
    " 'en die',\n",
    " 'en een',\n",
    " 'en het',\n",
    " 'en ik',\n",
    " 'en niet',\n",
    " 'enige',\n",
    " 'enkel',\n",
    " 'enkele',\n",
    " 'er een',\n",
    " 'er geen',\n",
    " 'er is',\n",
    " 'er niet',\n",
    " 'er nog',\n",
    " 'er ook',\n",
    " 'er zijn',\n",
    " 'erg',\n",
    " 'etc',\n",
    " 'eten',\n",
    " 'eu',\n",
    " 'europa',\n",
    " 'even',\n",
    " 'extra',\n",
    " 'feit',\n",
    " 'ga',\n",
    " 'gebeuren',\n",
    " 'gebruiken',\n",
    " 'gedaan',\n",
    " 'gedrag',\n",
    " 'geeft',\n",
    " 'gehad',\n",
    " 'geld',\n",
    " 'geleden',\n",
    " 'gelijk',\n",
    " 'gelukkig',\n",
    " 'gemaakt',\n",
    " 'genoeg',\n",
    " 'geval',\n",
    " 'geven',\n",
    " 'geweest',\n",
    " 'gezegd',\n",
    " 'gezien',\n",
    " 'goede',\n",
    " 'graag',\n",
    " 'griep',\n",
    " 'groep',\n",
    " 'grond',\n",
    " 'groot',\n",
    " 'grootste',\n",
    " 'grote',\n",
    " 'haar',\n",
    " 'had',\n",
    " 'hadden',\n",
    " 'halen',\n",
    " 'hand',\n",
    " 'hard',\n",
    " 'heb ik',\n",
    " 'heb je',\n",
    " 'hebben we',\n",
    " 'hebt',\n",
    " 'heel veel',\n",
    " 'helaas',\n",
    " 'hele',\n",
    " 'helemaal',\n",
    " 'helemaal niet',\n",
    " 'helpen',\n",
    " 'hem',\n",
    " 'het aantal',\n",
    " 'het een',\n",
    " 'het gaat',\n",
    " 'het kabinet',\n",
    " 'het land',\n",
    " 'het niet',\n",
    " 'het ook',\n",
    " 'het probleem',\n",
    " 'het rivm',\n",
    " 'het virus',\n",
    " 'het wel',\n",
    " 'hij',\n",
    " 'hoeveel',\n",
    " 'hoop',\n",
    " 'hoor',\n",
    " 'houden',\n",
    " 'huis',\n",
    " 'ic',\n",
    " 'idee',\n",
    " 'ieder',\n",
    " 'iedereen',\n",
    " 'iemand',\n",
    " 'iets',\n",
    " 'ik ben',\n",
    " 'ik denk',\n",
    " 'ik denk dat',\n",
    " 'ik heb',\n",
    " 'ik het',\n",
    " 'ik vind',\n",
    " 'in een',\n",
    " 'in het',\n",
    " 'in nederland',\n",
    " 'inderdaad',\n",
    " 'is dan',\n",
    " 'is dat',\n",
    " 'is de',\n",
    " 'is een',\n",
    " 'is en',\n",
    " 'is er',\n",
    " 'is geen',\n",
    " 'is niet',\n",
    " 'is ook',\n",
    " 'is voor',\n",
    " 'ja',\n",
    " 'jaar',\n",
    " 'jaren',\n",
    " 'je dat',\n",
    " 'je de',\n",
    " 'je een',\n",
    " 'je het',\n",
    " 'je niet',\n",
    " 'jij',\n",
    " 'juist',\n",
    " 'jullie',\n",
    " 'kabinet',\n",
    " 'kan je',\n",
    " 'kans',\n",
    " 'kant',\n",
    " 'keer',\n",
    " 'kijk',\n",
    " 'kijken',\n",
    " 'kinderen',\n",
    " 'kleine',\n",
    " 'klopt',\n",
    " 'kom',\n",
    " 'komt',\n",
    " 'krijgen',\n",
    " 'krijgt',\n",
    " 'kun',\n",
    " 'kun je',\n",
    " 'kunt',\n",
    " 'laat',\n",
    " 'laatste',\n",
    " 'land',\n",
    " 'landen',\n",
    " 'lang',\n",
    " 'laten',\n",
    " 'laten we',\n",
    " 'lees',\n",
    " 'lekker',\n",
    " 'leven',\n",
    " 'liggen',\n",
    " 'ligt',\n",
    " 'lijkt',\n",
    " 'lockdown',\n",
    " 'lopen',\n",
    " 'maakt',\n",
    " 'maanden',\n",
    " 'maar dat',\n",
    " 'maar de',\n",
    " 'maar een',\n",
    " 'maar het',\n",
    " 'maar ik',\n",
    " 'maatregelen',\n",
    " 'mag',\n",
    " 'maken',\n",
    " 'man',\n",
    " 'manier',\n",
    " 'me',\n",
    " 'media',\n",
    " 'meer dan',\n",
    " 'meeste',\n",
    " 'men',\n",
    " 'mening',\n",
    " 'mensen die',\n",
    " 'met de',\n",
    " 'met een',\n",
    " 'met het',\n",
    " 'meter',\n",
    " 'mijn',\n",
    " 'miljoen',\n",
    " 'minder',\n",
    " 'misschien',\n",
    " 'moet je',\n",
    " 'mogelijk',\n",
    " 'mogen',\n",
    " 'moment',\n",
    " 'mondkapje',\n",
    " 'mondkapjes',\n",
    " 'mooi',\n",
    " 'na',\n",
    " 'naar de',\n",
    " 'naar het',\n",
    " 'namelijk',\n",
    " 'natuur',\n",
    " 'natuurlijk',\n",
    " 'nederlandse',\n",
    " 'nee',\n",
    " 'nemen',\n",
    " 'net',\n",
    " 'net als',\n",
    " 'niemand',\n",
    " 'niet aan',\n",
    " 'niet alleen',\n",
    " 'niet dat',\n",
    " 'niet de',\n",
    " 'niet eens',\n",
    " 'niet in',\n",
    " 'niet meer',\n",
    " 'niet op',\n",
    " 'niet te',\n",
    " 'niet voor',\n",
    " 'niet zo',\n",
    " 'niets',\n",
    " 'nieuwe',\n",
    " 'nieuws',\n",
    " 'niks',\n",
    " 'nl',\n",
    " 'nodig',\n",
    " 'nog een',\n",
    " 'nog niet',\n",
    " 'nog steeds',\n",
    " 'nooit',\n",
    " 'normaal',\n",
    " 'nou',\n",
    " 'om de',\n",
    " 'om een',\n",
    " 'om het',\n",
    " 'om te',\n",
    " 'onder',\n",
    " 'onderzoek',\n",
    " 'ons',\n",
    " 'onze',\n",
    " 'onzin',\n",
    " 'ook de',\n",
    " 'ook een',\n",
    " 'ook niet',\n",
    " 'ook nog',\n",
    " 'op een',\n",
    " 'op het',\n",
    " 'op te',\n",
    " 'open',\n",
    " 'oplossing',\n",
    " 'over de',\n",
    " 'overheid',\n",
    " 'paar',\n",
    " 'partijen',\n",
    " 'pas',\n",
    " 'per',\n",
    " 'plaats',\n",
    " 'politie',\n",
    " 'politiek',\n",
    " 'praten',\n",
    " 'precies',\n",
    " 'prima',\n",
    " 'probleem',\n",
    " 'problemen',\n",
    " 'reactie',\n",
    " 'reden',\n",
    " 'regels',\n",
    " 'regering',\n",
    " 'rest',\n",
    " 'risico',\n",
    " 'rivm',\n",
    " 'rusland',\n",
    " 'rutte',\n",
    " 'scholen',\n",
    " 'situatie',\n",
    " 'snap',\n",
    " 'snel',\n",
    " 'soort',\n",
    " 'staan',\n",
    " 'staat',\n",
    " 'steeds',\n",
    " 'stemmen',\n",
    " 'stikstof',\n",
    " 'stoppen',\n",
    " 'straks',\n",
    " 'te doen',\n",
    " 'te gaan',\n",
    " 'te houden',\n",
    " 'te maken',\n",
    " 'te zijn',\n",
    " 'tegen',\n",
    " 'ten',\n",
    " 'terug',\n",
    " 'terwijl',\n",
    " 'testen',\n",
    " 'thuis',\n",
    " 'tijd',\n",
    " 'toe',\n",
    " 'toen',\n",
    " 'totaal',\n",
    " 'trump',\n",
    " 'tussen',\n",
    " 'twee',\n",
    " 'uit de',\n",
    " 'uitstoot',\n",
    " 'uw',\n",
    " 'vaak',\n",
    " 'vaccin',\n",
    " 'valt',\n",
    " 'van deze',\n",
    " 'van een',\n",
    " 'van het',\n",
    " 'vanaf',\n",
    " 'vanuit',\n",
    " 'vast',\n",
    " 'veel mensen',\n",
    " 'vele',\n",
    " 'verder',\n",
    " 'vind',\n",
    " 'vinden',\n",
    " 'virus',\n",
    " 'vlees',\n",
    " 'vol',\n",
    " 'volgens',\n",
    " 'volgens mij',\n",
    " 'voor een',\n",
    " 'voor het',\n",
    " 'vooral',\n",
    " 'vraag',\n",
    " 'waarom',\n",
    " 'waarschijnlijk',\n",
    " 'wanneer',\n",
    " 'want',\n",
    " 'waren',\n",
    " 'wat een',\n",
    " 'wat er',\n",
    " 'week',\n",
    " 'weet',\n",
    " 'weg',\n",
    " 'weinig',\n",
    " 'weken',\n",
    " 'wel een',\n",
    " 'welke',\n",
    " 'werd',\n",
    " 'wereld',\n",
    " 'werk',\n",
    " 'werken',\n",
    " 'werkt',\n",
    " 'wet',\n",
    " 'weten',\n",
    " 'wie',\n",
    " 'wij',\n",
    " 'wil',\n",
    " 'willen',\n",
    " 'wilt',\n",
    " 'word',\n",
    " 'zaken',\n",
    " 'zal',\n",
    " 'ze niet',\n",
    " 'zeer',\n",
    " 'zeg',\n",
    " 'zeggen',\n",
    " 'zegt',\n",
    " 'zeker',\n",
    " 'zelfs',\n",
    " 'zetten',\n",
    " 'zie',\n",
    " 'ziek',\n",
    " 'ziekenhuis',\n",
    " 'zien',\n",
    " 'ziet',\n",
    " 'zij',\n",
    " 'zijn de',\n",
    " 'zijn en',\n",
    " 'zijn er',\n",
    " 'zin',\n",
    " 'zit',\n",
    " 'zitten',\n",
    " 'zoals',\n",
    " 'zodat',\n",
    " 'zonder',\n",
    " 'zorg',\n",
    " 'zorgen',\n",
    " 'zouden',\n",
    " 'zoveel',\n",
    " 'zullen'\n",
    "]\n",
    "df1 = df1[df1.columns.intersection(correct_cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30252a19-d3e7-40c2-9bec-82a9c5eb5ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "################## RF_BoW ###################\n",
    "#############################################\n",
    "\n",
    "train_rfbow = train2[train2.columns.intersection(correct_cols)]\n",
    "test_rfbow = test[test.columns.intersection(correct_cols)]\n",
    "\n",
    "\n",
    "# Training on same grid as before\n",
    "rf = RandomForestClassifier()\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 3, scoring = 'f1', verbose=2, random_state=42, n_jobs = -1)\n",
    "rf_random = rf_random.fit(train_rfbow, train_labels)\n",
    "rf1 = rf_random.best_estimator_\n",
    "\n",
    "# Evaluation on Test set\n",
    "pred = rf1.predict(test_rfbow)\n",
    "print('f1 on test set (RF_BoW):',f1_score(test_labels_bow, list(pred), pos_label=True))\n",
    "print('prec on test set (RF_BoW):',precision_score(test_labels_bow, list(pred), pos_label=True))\n",
    "print('rec on test set (RF_BoW):',recall_score(test_labels_bow, list(pred), pos_label=True))\n",
    "\n",
    "\n",
    "# Save rf_BoW\n",
    "with open('rf_bow', 'wb') as a:\n",
    "    pickle.dump(rf1, a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aca8fa1-c8bb-4692-85ff-2a65598072da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "################## RobBERT  #################\n",
    "#############################################\n",
    "\n",
    "# Get texts only\n",
    "train_texts = list(train2['content'])\n",
    "validate_texts = list(validate['content'])\n",
    "test_texts = list(test['content'])\n",
    "\n",
    "\n",
    "# Names\n",
    "target_names = ['Not_feat', 'Feat']\n",
    "\n",
    "# Tokenizing\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"pdelobelle/robBERT-base\")\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=205)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=205)\n",
    "validate_encodings = tokenizer(validate_texts, truncation=True, padding=True, max_length=205)\n",
    "\n",
    "# Combining datasets\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# convert our tokenized data into a torch Dataset\n",
    "train_dataset = Dataset(train_encodings, train_labels_num)\n",
    "test_dataset = Dataset(test_encodings, test_labels_num)\n",
    "validate_dataset = Dataset(validate_encodings, validate_labels_num)\n",
    "\n",
    "# Load base model\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"pdelobelle/robBERT-base\",num_labels=len(target_names))\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    precision = precision_score(labels, preds)\n",
    "    recall = recall_score(labels, preds)\n",
    "    \n",
    "    return {\n",
    "      'accuracy': acc,\n",
    "      'f1': f1,  \n",
    "      'precision': precision,\n",
    "      'recall': recall  \n",
    "          }\n",
    "# Specifics for training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results/feat_rec',          \n",
    "    num_train_epochs=10,              \n",
    "    per_device_train_batch_size=64,  \n",
    "    per_device_eval_batch_size=64,   \n",
    "    warmup_steps=250, \n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,               \n",
    "    logging_dir='./logs/feat_rec',            \n",
    "    load_best_model_at_end=True,    \n",
    "    metric_for_best_model='f1',\n",
    "    logging_steps=500,               \n",
    "    evaluation_strategy=\"steps\",     \n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,        \n",
    "    eval_dataset=validate_dataset,         \n",
    "    compute_metrics=compute_metrics, \n",
    ")\n",
    "# Gpu\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# And training itself\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "model_path = \"featrec\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "\n",
    "# On test set:\n",
    "def get_prediction(text):\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, max_length=205, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs[0].softmax(1)\n",
    "    return probs.tolist()\n",
    "\n",
    "test_labels2 = []\n",
    "for i in range(len(test_labels_num)):\n",
    "    if test_labels_num[i]==0:\n",
    "        test_labels2.append('Non_feat')\n",
    "    else:\n",
    "        test_labels2.append('Feat')\n",
    "\n",
    "nf = []\n",
    "f = []\n",
    "\n",
    "\n",
    "for i in range(len(test_texts)):\n",
    "    temp = get_prediction(test_texts[i])\n",
    "    temp = temp[0]\n",
    "    nf.append(temp[0])\n",
    "    f.append(temp[1])\n",
    "    \n",
    "pred = pd.DataFrame(nf)\n",
    "pred['featured'] = f\n",
    "pred = pred.rename(columns={0: 'Non_feat', 'featured': 'Feat'})\n",
    "pred_label = []\n",
    "for row in pred.itertuples():\n",
    "    if row.Non_feat > row.Feat:\n",
    "        pred_label.append('Non_feat')\n",
    "    else:\n",
    "        pred_label.append('Feat')\n",
    "        \n",
    "pred['Pred_label'] = pred_label\n",
    "pred['Label'] = test_labels2\n",
    "\n",
    "\n",
    "print('prec on test set (RobBERT):', precision_score(pred['Label'], pred['Pred_label'], pos_label='Feat'))\n",
    "print('recall on test set(RobBERT):', recall_score(pred['Label'], pred['Pred_label'], pos_label='Feat'))\n",
    "print('f1 on test set(RobBERT):',f1_score(pred['Label'], pred['Pred_label'], pos_label='Feat'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36e6960-149e-4529-b1cf-738b4b4763a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "################## RF_emb ###################\n",
    "#############################################\n",
    "\n",
    "\n",
    "# Getting CLS tokens from train and test\n",
    "\n",
    "\n",
    "word_embedding_model = models.Transformer(\"featrec\")\n",
    "\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=False,\n",
    "                               pooling_mode_cls_token=True,\n",
    "                               pooling_mode_max_tokens=False)\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "train_texts_emb = list(train2.content)\n",
    "test_texts_emb = list(test.content)\n",
    "\n",
    "print('encoding')\n",
    "tr_emb = []\n",
    "for i in range(len(train_texts_emb)):\n",
    "    temp = model.encode(train_texts_emb[i])\n",
    "    tr_emb.append(temp)\n",
    "print('train done')\n",
    "test_emb = model.encode(test_texts_emb)\n",
    "\n",
    "train_emb= pd.DataFrame(tr_emb)\n",
    "test_emb = pd.DataFrame(test_emb)\n",
    "\n",
    "# Combining CLS tokens with non-textual features\n",
    "test_rfemb = test.drop(test.columns[0], axis=1)\n",
    "train_rfemb = train2.drop(train.columns[0], axis=1)\n",
    "train_rfemb = pd.concat([train_rfemb, train_emb], axis=1)\n",
    "test_rfemb = pd.concat([test_rfemb, test_emb], axis=1)\n",
    "\n",
    "# Save for replication\n",
    "train2.to_csv('train_emb_full.csv')\n",
    "test2.to_csv('test_emb_full.csv')\n",
    "\n",
    "# Dropping unnecessary features\n",
    "train_labels_emb = train2.featured\n",
    "test_labels_emb= test2.featured\n",
    "train_rfemb = train_rfemb.drop('featured', axis=1)\n",
    "train_rfemb = train_rfemb.drop('content', axis=1)\n",
    "test_rfemb = test_rfemb.drop('featured', axis=1)\n",
    "test_rfemb = test_rfemb.drop('content', axis=1)\n",
    "\n",
    "# Training\n",
    "train2.columns = train2.columns.map(str)\n",
    "test2.columns = test2.columns.map(str)\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 3, scoring = 'f1', verbose=2, random_state=42, n_jobs = -1)\n",
    "rf1 = rf_random.fit(train_rfemb, train_labels_emb)\n",
    "pred = rf1.predict(test_rfemb)\n",
    "print('f1 on test set (RF_emb):',f1_score(test_labels_emb, list(pred), pos_label=True))\n",
    "print('prec on test set (RF_emb):',precision_score(test_labels_emb, list(pred), pos_label=True))\n",
    "print('rec on test set (RF_emb):',recall_score(test_labels_emb, list(pred), pos_label=True))\n",
    "\n",
    "# Save RF_EMB\n",
    "with open('rf_emb', 'wb') as a:\n",
    "    pickle.dump(rf1, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7231d46-da89-46d6-8632-574714ee5b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "###### BASELINE #########\n",
    "##########################\n",
    "\n",
    "\n",
    "baseline_pred = []\n",
    "for row in test.itertuples():\n",
    "    if row.ratio_featured>0.03:\n",
    "        baseline_pred.append(True)\n",
    "    else:\n",
    "        baseline_pred.append(False)\n",
    "print('f1 on test set(baseline):',f1_score(test_labels, baseline_pred, pos_label=True))\n",
    "print('prec on test set (baseline):',precision_score(test_labels, baseline_pred, pos_label=True))\n",
    "print('rec on test set (baseline):',recall_score(test_labels, baseline_pred, pos_label=True))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0001c509-8de1-454c-a79d-28cd58a18bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "############## SVM #################\n",
    "#####################################\n",
    "\n",
    "\n",
    "svm = make_pipeline(StandardScaler(), SVC(gamma='auto', probability=True))\n",
    "svm.fit(train_rf, train_labels)\n",
    "\n",
    "test = test.drop('featured', axis=1)\n",
    "pred = svm.predict(test_rf)\n",
    "print('f1 on test set (SVM):',f1_score(test_labels, list(pred), pos_label=True))\n",
    "print('prec on test set (SVM):',precision_score(test_labels, list(pred), pos_label=True))\n",
    "print('rec on test set (SVM):',recall_score(test_labels, list(pred), pos_label=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c7c0af-e88f-4df1-8f0d-1a9250255d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "########## CNN ##############\n",
    "############################\n",
    "\n",
    "embedding_dim = 100  \n",
    "vocab_size = len(tokenizer.word_index) + 1 \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim)) #, input_length=max_sequence_length -> deprecated\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(padded_sequences_train, y_train, epochs=10, batch_size=32, validation_data=(padded_sequences_val, y_val))\n",
    "\n",
    "predictions = model.predict(padded_sequences_test)\n",
    "\n",
    "\n",
    "preds = []\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i]>0.5:\n",
    "        preds.append(1)\n",
    "    else:\n",
    "        preds.append(0)\n",
    "\n",
    "\n",
    "print('f1 on test set: (CNN)',f1_score(list(y_test), preds, pos_label=1))\n",
    "print('prec on test set: (CNN)',precision_score(list(y_test), preds, pos_label=1))\n",
    "print('rec on test set: (CNN)',recall_score(list(y_test), preds, pos_label=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eb7c01-5925-4716-aad4-a7c6f7cebd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "##### BiLSTM ################\n",
    "############################\n",
    "\n",
    "\n",
    "max_length = 100 \n",
    "input_layer = Input(shape=(max_length,))\n",
    "embedding_layer = Embedding(vocab_size, 100)(input_layer)\n",
    "bilstm_layer = Bidirectional(LSTM(64))(embedding_layer)\n",
    "output_layer = Dense(1, activation='sigmoid')(bilstm_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "\n",
    "\n",
    "model.fit(padded_sequences_train, y_train, epochs=10, batch_size=32, validation_data=(padded_sequences_val, y_val))\n",
    "\n",
    "predictions = model.predict(padded_sequences_test)\n",
    "preds = []\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i]>0.5:\n",
    "        preds.append(1)\n",
    "    else:\n",
    "        preds.append(0)\n",
    "\n",
    "\n",
    "print(c.Counter(preds))\n",
    "\n",
    "\n",
    "print('f1 on test set: (biLSTM)',f1_score(list(y_test), preds, pos_label=1))\n",
    "print('prec on test set: (biLSTM):',precision_score(list(y_test), preds, pos_label=1))\n",
    "print('rec on test set: (biLSTM):',recall_score(list(y_test), preds, pos_label=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
